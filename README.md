# MIT-18.065-Notes
Lecture Notes of "Matrix Methods in Data Analysis, Signal Processing, and Machine Learning"

Course : **Matrix Methods in Data Analysis, Signal Processing, and Machine Learning**

Source : https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/index.htm

**Lecture 15**: Matrices A(t) Depending on t, Derivative = dA/dt <br />
**Lecture 16**: Derivatives of Inverse and Singular Values <br />
**Lecture 17**: Rapidly Decreasing Singular Values <br />
**Lecture 18**: Counting Parameters in SVD, LU, QR, Saddle Points <br />
**Lecture 19**: Saddle Points Continued, Maxmin Principle <br />
**Lecture 20**: Definitions and Inequalities <br />
**Lecture 21**: Minimizing a Function Step by Step <br />
**Lecture 22**: Gradient Descent: Downhill to a Minimum <br />
**Lecture 23**: Accelerating Gradient Descent (Use Momentum) <br />
**Lecture 24**: Linear Programming and Two-Person Games <br />
**Lecture 25**: Stochastic Gradient Descent <br />
**Lecture 26**: Structure of Neural Nets for Deep Learning <br />
**Lecture 27**: Backpropogation: Find Partial Derivatives <br />
**Lecture 30**: Completing a Rank One Matrix, Circulants! <br />
**Lecture 31**: Eigenvectors of Circulant Matrices: Fourier Matrix <br />
**Lecture 32**: ImageNet is a Convolutional Neural Network (CNN), The Convolution Rule <br />
**Lecture 33**: Neural Nets and the Learning Function <br />
**Lecture 34**: Distance Matrices, Procrustes Problem <br />
**Lecture 35**: Finding Clusters in Graphs <br />
