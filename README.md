# MIT-18.065-Notes
Lecture Notes of "Matrix Methods in Data Analysis, Signal Processing, and Machine Learning"

Course : **Matrix Methods in Data Analysis, Signal Processing, and Machine Learning**

Source : https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/index.htm

**Lecture 15**: Matrices A(t) Depending on t, Derivative = dA/dt <br />
**Lecture 16**: Derivatives of Inverse and Singular Values
**Lecture 17**: Rapidly Decreasing Singular Values
**Lecture 18**: Counting Parameters in SVD, LU, QR, Saddle Points
**Lecture 19**: Saddle Points Continued, Maxmin Principle
**Lecture 20**: Definitions and Inequalities
**Lecture 21**: Minimizing a Function Step by Step
**Lecture 22**: Gradient Descent: Downhill to a Minimum
**Lecture 23**: Accelerating Gradient Descent (Use Momentum)
**Lecture 24**: Linear Programming and Two-Person Games
**Lecture 25**: Stochastic Gradient Descent
**Lecture 26**: Structure of Neural Nets for Deep Learning
**Lecture 27**: Backpropogation: Find Partial Derivatives
**Lecture 30**: Completing a Rank One Matrix, Circulants!
**Lecture 31**: Eigenvectors of Circulant Matrices: Fourier Matrix
**Lecture 32**: ImageNet is a Convolutional Neural Network (CNN), The Convolution Rule
**Lecture 33**: Neural Nets and the Learning Function
**Lecture 34**: Distance Matrices, Procrustes Problem
**Lecture 35**: Finding Clusters in Graphs
